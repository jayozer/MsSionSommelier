{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# seq2seq: \n",
    "### I think generating a new sequence that can be a possible ending of a sentence is a little close to what I am looking for. Ideally I woud like to be able to apply this to car share banner generation. And my goal is to be able to write in a way that can end the input sentence. SInce this is sentence completion, I would like to have has many variety in my result set. i know I dont have 120M images like resnet however I do have 0.1% of that ~ 120K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install spacy\n",
    "# !python3 -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/__init__.py:1473: The name tf.estimator.inputs is deprecated. Please use tf.compat.v1.estimator.inputs instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, LSTM\n",
    "from keras import backend\n",
    "\n",
    "import numpy as np\n",
    "import string\n",
    "\n",
    "import os\n",
    "os.getcwd()\n",
    "\n",
    "import re\n",
    "from colorama import Fore, Back, Style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/ec2-user/SageMaker/MSDS696/wine_msds.pkl', 'rb') as f:\n",
    "    wine_msds = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unknown Taster        24917\n",
       "Roger Voss            23560\n",
       "Michael Schachner     14046\n",
       "Kerin O’Keefe          9697\n",
       "Paul Gregutt           8868\n",
       "Virginie Boone         8708\n",
       "Matt Kettmann          5730\n",
       "Joe Czerwinski         4766\n",
       "Sean P. Sullivan       4461\n",
       "Anna Lee C. Iijima     4017\n",
       "Jim Gordon             3766\n",
       "Anne Krebiehl MW       3290\n",
       "Lauren Buzzeo          1700\n",
       "Susan Kostrzewa        1022\n",
       "Mike DeSimone           461\n",
       "Jeff Jenssen            436\n",
       "Alexander Peartree      383\n",
       "Carrie Dykes            129\n",
       "Fiona Adams              24\n",
       "Christina Pickard         6\n",
       "Name: taster_name, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wine_msds['taster_name'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wont work with entire set. Making source smaller. If I have the chance, I will try with entire corpus because I will have to create a completely different type of machine, working on this at the same tme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(436, 13)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tasters = ['Roger Voss', 'Michael Schachner'] # still too much memory 228 is not possible for me \n",
    "#tasters = ['Jim Gordon'] # this is 10 times less.\n",
    "#tasters = ['Virginie Boone']  # still 180gb ram \n",
    "tasters = ['Jeff Jenssen']\n",
    "\n",
    "wine_msds_taster = wine_msds[wine_msds.taster_name.isin(tasters)]\n",
    "wine_msds_taster.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(436, 1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>98828</th>\n",
       "      <td>This blend of Cabernet Sauvignon and Merlot ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5961</th>\n",
       "      <td>This Slovenian Chardonnay has aromas of orange...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6968</th>\n",
       "      <td>This amber colored, orange-style wine has arom...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             description\n",
       "98828  This blend of Cabernet Sauvignon and Merlot ha...\n",
       "5961   This Slovenian Chardonnay has aromas of orange...\n",
       "6968   This amber colored, orange-style wine has arom..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_rev = wine_msds_taster[['description']].drop_duplicates()\n",
    "\n",
    "print(df_rev.shape)\n",
    "df_rev.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First some regex to get rid of weird words and replace sentence endings.\n",
    "def normalize_text(text):\n",
    "    tm1 = re.sub('[0-9]', '', text, flags=re.DOTALL)\n",
    "    tm2 = re.sub('\\x9d', ' ', tm1, flags=re.DOTALL)\n",
    "    tm3 = re.sub('\\xa0', ' ', tm2, flags=re.DOTALL)\n",
    "    tm4 = re.sub('•', '', tm3, flags=re.DOTALL)  # extra in case need additional\n",
    "    tm5 = re.sub('\\xad', ' ', tm4, flags=re.DOTALL)\n",
    "    tm6 = re.sub('\\u3000', ' ', tm5, flags=re.DOTALL)\n",
    "    tm7 = re.sub('`', '', tm6, flags=re.DOTALL) # had to add these, for tm9 anything after would not work\n",
    "    tm8 = re.sub('´', '', tm7, flags=re.DOTALL)\n",
    "    tm9 = re.sub('<[^>]…+>–©¡—¨¬°º½%$.-•', '', tm8, flags=re.DOTALL)\n",
    "    return tm9.replace(\"\\n\", \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case it works: '\\\\' was giving me so much trouble. one \\ excapes the other. since it is looking for aingle one, removing one and trying my chances.idea is if I remove one \\ then would remove all.\n",
    "No it did not, I was not able to remove \\\\ what gives? - need to check this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing code syntax from text \n",
    "df_rev['description_clean']=df_rev['description'].apply(normalize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I am not able to remove everything using regex, so doing good old fashion\n",
    "spec_chars = [\"!\",'\"',\"#\",\"%\",\"&\",\"'\",\"(\",\")\",\n",
    "              \"*\",\"+\",\",\",\"-\",\".\",\"/\",\":\",\";\",\"<\",\n",
    "              \"=\",\">\",\"?\",\"@\",\"[\",\"\\\\\",\"]\",\"^\",\"_\", \"‘\", \"’\", \"“\", \"”\", \"…\",\n",
    "              \"`\",\"{\",\"|\",\"}\",\"~\",\"–\", \"©\",'¡',\"—\",\"¨\",\"¬\",\"°\",\"º\",\"½\",\"%\",\"$\",\".\",\"-\",\"•\"]\n",
    "for char in spec_chars:\n",
    "    df_rev['description_clean'] = df_rev['description_clean'].str.replace(char, '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98828     This blend of Cabernet Sauvignon and Merlot ha...\n",
      "5961      This Slovenian Chardonnay has aromas of orange...\n",
      "6968      This amber colored, orange-style wine has arom...\n",
      "6969      This wine is garnet in color, with aromas of c...\n",
      "15311     Aromas of freshly mowed grass, lemon pith, and...\n",
      "                                ...                        \n",
      "70305     This deep straw-colored wine has a nose of man...\n",
      "100610    This dry Riesling offers rich floral aromas of...\n",
      "100615    Made with 100% Cabernet Moravia (an indigenous...\n",
      "100616    On the nose, spice outweighs fruit, with notes...\n",
      "91173     A mix of red and black fruits pervade on the n...\n",
      "Name: description, Length: 436, dtype: object\n",
      "\u001b[32m\u001b[2m\n",
      "After normalizing text-----\n",
      "\n",
      "98828     This blend of Cabernet Sauvignon and Merlot ha...\n",
      "5961      This Slovenian Chardonnay has aromas of orange...\n",
      "6968      This amber colored orangestyle wine has aromas...\n",
      "6969      This wine is garnet in color with aromas of ch...\n",
      "15311     Aromas of freshly mowed grass lemon pith and l...\n",
      "                                ...                        \n",
      "70305     This deep strawcolored wine has a nose of mang...\n",
      "100610    This dry Riesling offers rich floral aromas of...\n",
      "100615    Made with  Cabernet Moravia an indigenous cros...\n",
      "100616    On the nose spice outweighs fruit with notes o...\n",
      "91173     A mix of red and black fruits pervade on the n...\n",
      "Name: description_clean, Length: 436, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df_rev['description'])\n",
    "print(Fore.GREEN + Style.DIM + '\\nAfter normalizing text-----\\n')\n",
    "print(df_rev['description_clean'])\n",
    "\n",
    "# Nothing visible below but text had abnormalities that had to be treated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I will keep the corpus as is before I clean it further. A few wine variety names should not hurt the overall intend and again my intention is to be coherent but not homogenous. With that said the special chars almost always muddle the intent. I am not talking about removing the exclamation mark (!), just the weird chars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_list_to_string(org_list, seperator=' '):\n",
    "    \"\"\" Convert list to string, by joining all item in list with given separator.\n",
    "        Returns the concatenated string \"\"\"\n",
    "    return seperator.join(org_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 36\n",
      "[' ', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'á', 'ä', 'è', 'é', 'ñ', 'ú', 'û', 'ü', 'ž']\n"
     ]
    }
   ],
   "source": [
    "# Trick: A way to check unique vocabulary\n",
    "wine_review_corpus = \" \".join(df_rev['description_clean'].astype(str))\n",
    "# Convert list of strings to string\n",
    "check_vocabulary = convert_list_to_string(wine_review_corpus).lower()\n",
    "vocabulary = sorted(set(check_vocabulary)) # get rid of duplicates\n",
    "print('Vocabulary size:', len(vocabulary))\n",
    "print(vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay we are rolling with this ^. 55 as vocabulary size is decent. RNN example showed us it is nice to have this type of variety however in full sentance generation not sure how it is going to work. \n",
    "\n",
    "Update: 50 chars now, Instead of using 119K, I am planning to imitate top 2 reviewers, their total number of reviews come out to ~37K. About 1/3 of original.\n",
    "\n",
    "Update: Still needed 228 memory even at 8bits, so I decided to take 10% of previous run, at 3.7% of total reviews our new robot is now 'Jim Gordon'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description</th>\n",
       "      <th>description_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>98828</th>\n",
       "      <td>This blend of Cabernet Sauvignon and Merlot ha...</td>\n",
       "      <td>This blend of Cabernet Sauvignon and Merlot ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5961</th>\n",
       "      <td>This Slovenian Chardonnay has aromas of orange...</td>\n",
       "      <td>This Slovenian Chardonnay has aromas of orange...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6968</th>\n",
       "      <td>This amber colored, orange-style wine has arom...</td>\n",
       "      <td>This amber colored orangestyle wine has aromas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6969</th>\n",
       "      <td>This wine is garnet in color, with aromas of c...</td>\n",
       "      <td>This wine is garnet in color with aromas of ch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15311</th>\n",
       "      <td>Aromas of freshly mowed grass, lemon pith, and...</td>\n",
       "      <td>Aromas of freshly mowed grass lemon pith and l...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             description  \\\n",
       "98828  This blend of Cabernet Sauvignon and Merlot ha...   \n",
       "5961   This Slovenian Chardonnay has aromas of orange...   \n",
       "6968   This amber colored, orange-style wine has arom...   \n",
       "6969   This wine is garnet in color, with aromas of c...   \n",
       "15311  Aromas of freshly mowed grass, lemon pith, and...   \n",
       "\n",
       "                                       description_clean  \n",
       "98828  This blend of Cabernet Sauvignon and Merlot ha...  \n",
       "5961   This Slovenian Chardonnay has aromas of orange...  \n",
       "6968   This amber colored orangestyle wine has aromas...  \n",
       "6969   This wine is garnet in color with aromas of ch...  \n",
       "15311  Aromas of freshly mowed grass lemon pith and l...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_rev.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lower case, and change type to a list. \n",
    "I have seen examples where upper case exists in the middle of the word which is a simpler mistake and can give away the bot behind the wine review. For this reason lowering the case across."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this blend of cabernet sauvignon and merlot has aromas of coffee black cherry and dark chocolate it is tart in the mouth with flavors of cherry and red raspberry',\n",
       " 'this slovenian chardonnay has aromas of orange vanilla toasted almonds and citrus blossom the mouth delivers flavors of honeydew melon pear and apple before a crisp acidic finish',\n",
       " 'this amber colored orangestyle wine has aromas of canned peaches and apricots on the nose while fresh stone fruits seem to dominate the full bodied palate the finish is creamy and lingers on',\n",
       " 'this wine is garnet in color with aromas of cherry vanilla red plum and cassis flavors of tart cherry red raspberry and blackberry drive the palate and lead to a lingering finish',\n",
       " 'aromas of freshly mowed grass lemon pith and lemon grass open for mineraldriven flavors of bartlett pear lemon juice and green apple in the midpalate',\n",
       " 'this slovenian ribolla gialla has aromas of peach and vanilla with flavors of freshly baked peach pie canned apricots and green apple it is soft on entry rounded in the mouth and shows good minerality on the finish',\n",
       " 'crafted in an oxidative style this wine is light brown to the eye with aromas of toasted nuts and caramelized pineapple flavors of hazelnut toasted almond and lemon sorbet continue through the smooth finish that ends on a zesty note',\n",
       " 'a straw yellow color with pink tinges this bright wine is zesty and juicy at the same time it has a round body and full mouthfeel with flavors of green apple bartlett pear and citrus blossoms theres bracing limejuice acidity on the finish',\n",
       " 'known in italy as ribolla gialla rebula is the name used for the same grape on the slovenian side of the border this wine has aromas of pineapple tropical fruit and dried lavender flavors of green apple and pineapple flourish on the palate lingering into the surprisingly tart finish',\n",
       " 'this sweet wine is nut brown in color with aromas of dried prunes caramelized pears and raisins the palate is driven by sweet dried fruit tones balanced by bright acidity']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_rev['description_clean'] = df_rev['description_clean'].str.lower()  \n",
    "reviews_corpus = df_rev['description_clean'].tolist()\n",
    "reviews_corpus[:10]\n",
    "\n",
    "# I need to do a better job on the special chars. This method is not working like I like, \n",
    "# I can add missing ones to another method. - I did, all done!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess sentences:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess sentences so they can be used to train the encoder decoder model. Find the input and the target sequence from original sentences. Similar to what I have been doing. Instead of each word, this time each sentence is divided into two at each char position. This generates one start and one end for each occurence. Just like the /n and /t from RNN run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Generate prefix and suffix sentences\n",
    "prefix_sentences = []\n",
    "suffix_sentences = []\n",
    "# Iterate over each character position in each email\n",
    "for reviews in reviews_corpus: \n",
    "    for index in range(len(reviews)): \n",
    "        # Find the prefix and suffix        \n",
    "        prefix = reviews[:index+1]        \n",
    "        suffix = '\\t' + reviews[index+1:] + '\\n'\n",
    "        \n",
    "        # Add prefix and sufix to the lists        \n",
    "        prefix_sentences.append(prefix)        \n",
    "        suffix_sentences.append(suffix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "89223"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(prefix_sentences) # just curious. This worked with 150GB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vocabulary and mappings\n",
    "# Initialize vocabulary with the start and end token\n",
    "vocabulary = set(['\\t', '\\n'])\n",
    "\n",
    "# Check each char in each email by iterating for each char in each email\n",
    "for reviews in reviews_corpus:\n",
    "    for char in reviews:\n",
    "        if (char not in vocabulary): \n",
    "            vocabulary.add(char) \n",
    "\n",
    "# Sort the vocabulary\n",
    "vocabulary = sorted(vocabulary)  \n",
    "# Tips: initially I had sorted(list(vocabulary)) \n",
    "# I end up getting AttributeError: 'list' object has no attribute 'add'\n",
    "            \n",
    "\n",
    "# Create char to int and int to char mapping\n",
    "char_to_idx = dict((char, idx) for idx, char in enumerate(vocabulary))\n",
    "idx_to_char = dict((idx, char) for idx, char in enumerate(vocabulary))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vocabulary and the character to integer and integer to character mapping are done. Next is creating input and target vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define input and target vectors\n",
    "# Find the length of the longest prefix and suffix\n",
    "max_len_prefix_sent = max([len(prefix) for prefix in prefix_sentences])\n",
    "max_len_suffix_sent = max([len(suffix) for suffix in suffix_sentences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a 3-D zero vector for the prefix sentences\n",
    "input_data_prefix = np.zeros((len(prefix_sentences), max_len_prefix_sent, len(vocabulary)), dtype='float32')\n",
    "# Define a 3-D zero vector for the suffix sentences\n",
    "input_data_suffix = np.zeros((len(suffix_sentences), max_len_suffix_sent, len(vocabulary)), dtype='float32')\n",
    "# Define a 3-D zero vector for the target data\n",
    "target_data = np.zeros((len(suffix_sentences), max_len_suffix_sent, len(vocabulary)), dtype='float32')\n",
    "\n",
    "# MemoryError: Unable to allocate 4.61 TiB for an array with shape (28063433, 792, 57) and data type float32\n",
    "\n",
    "# Run 2, added 5.5TiB. Love the cloud! Still not working though. lol. \n",
    "\n",
    "# difference between dtype='uint8' and float32 is exactly what it is 4 times! \n",
    "\n",
    "\n",
    "# Here I filled the input and target vectors with proper shape, next is filling the vectors with data\n",
    "# I will iterate over each char of each sentence and convert the char to a one-hot encoded vector\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input and target vectors are now available. Now fill the vectors with data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize input and target vectors\n",
    "for i in range(len(prefix_sentences)):\n",
    "    # Iterate over each character in each prefix\n",
    "    for k, ch in enumerate(prefix_sentences[i]):\n",
    "        # Convert the character to a one-hot encoded vector\n",
    "        input_data_prefix[i, k, char_to_idx[ch]] = 1\n",
    "        \n",
    "    # Iterate over each character in each suffix\n",
    "    for k, ch in enumerate(suffix_sentences[i]):\n",
    "        # Convert the character to a one-hot encoded vector\n",
    "        input_data_suffix[i, k, char_to_idx[ch]] = 1\n",
    "\n",
    "        # Target data is one timestep ahead and excludes start character\n",
    "        if k > 0:\n",
    "            target_data[i, k-1, char_to_idx[ch]] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence autocompletion using Encoder-Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Input\n",
    "from keras.models import load_model\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Encoder:\n",
    "\n",
    "# Create the input layer of the encoder\n",
    "encoder_input = Input(shape=(None, len(vocabulary)))\n",
    "# Create LSTM Layer of size 256\n",
    "encoder_LSTM = LSTM(256, return_state = True)\n",
    "# Save encoder output, hidden and cell state\n",
    "encoder_outputs, encoder_h, encoder_c = encoder_LSTM(encoder_input)\n",
    "# Save encoder states\n",
    "encoder_states = [encoder_h, encoder_c]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create decoder input layer\n",
    "decoder_input = Input(shape=(None, len(vocabulary)))\n",
    "\n",
    "# Create LSTM layer of size 256\n",
    "decoder_LSTM = LSTM(256,return_sequences=True, return_state = True)\n",
    "\n",
    "# Save decoder output\n",
    "decoder_out, _ , _ = decoder_LSTM(decoder_input, initial_state=encoder_states)\n",
    "\n",
    "# Create a `Dense` layer with softmax activation\n",
    "decoder_dense = Dense(len(vocabulary),activation='softmax')\n",
    "\n",
    "# Save the decoder output\n",
    "decoder_out = decoder_dense(decoder_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, None, 38)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, None, 38)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, 256), (None, 302080      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   [(None, None, 256),  302080      input_2[0][0]                    \n",
      "                                                                 lstm_1[0][1]                     \n",
      "                                                                 lstm_1[0][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, None, 38)     9766        lstm_2[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 613,926\n",
      "Trainable params: 613,926\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Combine the encoder and the decoder\n",
    "\n",
    "# Build model\n",
    "model = Model(inputs=[encoder_input, decoder_input], outputs=[decoder_out])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
    "\n",
    "# Print model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the LSTM Network (to generate suffixes corresponding to the prefix sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:2741: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "Train on 71378 samples, validate on 17845 samples\n",
      "Epoch 1/1\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:184: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "71378/71378 [==============================] - 2312s 32ms/step - loss: 0.4352 - val_loss: 0.3898\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fc7b03f5f98>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the model\n",
    "model.fit(x=[input_data_prefix, input_data_suffix], y=target_data,\n",
    "          batch_size=64, epochs=1, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run 1: Train on 71378 samples, validate on 17845 samples. My first run was super basic. At a small data set ~3.5% of the overall corpus. Only one reviewer. The amount of processing required is immense and also I am pretty sure I am not doing it the most efficient way but again that does not take away the resources required.One epoch at batch size 64 at a loss of 0.87 took a little over 2 hrs. Small instances mean nothing to text data. (Even for th emost basic run I had to use a AWS c5_4xlarge instance. What is interesting is that loss ratio started at 0.72 and now at 0.52 with past halfway mark. Finished at 2312s 32ms/step - loss:04352 - val loss: 0.3\n",
    "\n",
    "Run 2: batch size 128 to Epoch 20 (Why 20, from research I see that 20 is a good threshold for smaller data sets such as this one. \n",
    "\n",
    "Run 3: batch size 128 to Epoch 500 - reason is to lower loss value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Inference Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create encoder inference model\n",
    "encoder_model_inf = Model(encoder_input, encoder_states)\n",
    "\n",
    "# Create decoder input states for inference\n",
    "decoder_state_input_h = Input(shape=(256,))\n",
    "decoder_state_input_c = Input(shape=(256,))\n",
    "decoder_input_states = [decoder_state_input_h, decoder_state_input_c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get decoder output and feed it to the dense layer for final output prediction\n",
    "decoder_out, decoder_h, decoder_c = decoder_LSTM(decoder_input, initial_state=decoder_input_states)\n",
    "decoder_states = [decoder_h , decoder_c]\n",
    "decoder_out = decoder_dense(decoder_out)\n",
    "\n",
    "# Create decoder inference model\n",
    "decoder_model_inf = Model(inputs=[decoder_input] + decoder_input_states, outputs=[decoder_out] + decoder_states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict the first character using inference models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass input prefix to the Encoder inference model and get the states\n",
    "inp_seq = input_data_prefix[4:5]\n",
    "states_val = encoder_model_inf.predict(inp_seq)\n",
    "\n",
    "# Seed the first character and get output from the decoder \n",
    "target_seq = np.zeros((1, 1, len(vocabulary)))\n",
    "target_seq[0, 0, char_to_idx['\\t']] = 1  \n",
    "# get output from the decoder inference model\n",
    "decoder_out, decoder_h, decoder_c = decoder_model_inf.predict(x=[target_seq] + states_val)\n",
    "\n",
    "# Find out the most probable next character from the Decoder output\n",
    "max_val_index = np.argmax(decoder_out[0,-1,:])\n",
    "sampled_suffix_char = idx_to_char[max_val_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    }
   ],
   "source": [
    "# Print the first character\n",
    "print(sampled_suffix_char)\n",
    "\n",
    "\n",
    "# Super hilarious, first time I run this my first char returned as space. Imagine the confusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict the second Character\n",
    "This is possible by appendding the first character to the existing input to generate the second character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n\n"
     ]
    }
   ],
   "source": [
    "# Insert the generated character from last time to the target sequence \n",
    "target_seq = np.zeros((1, 1, len(vocabulary)))\n",
    "target_seq[0, 0, max_val_index] = 1\n",
    "\n",
    "# Initialize the decoder state to the states from last iteration\n",
    "states_val = [decoder_h, decoder_c]\n",
    "\n",
    "# Get decoder output\n",
    "decoder_out, decoder_h, decoder_c = decoder_model_inf.predict(x=[target_seq] + states_val)\n",
    "\n",
    "# Get most probable next character and print it.\n",
    "max_val_index = np.argmax(decoder_out[0,-1,:])\n",
    "sampled_suffix_char = idx_to_char[max_val_index]\n",
    "print(sampled_suffix_char)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auto generate entire sentences\n",
    "I can give the starting letter and get first and second letter, all that needs to be done is to repeat this process. until I get the end tokem to generate the full suffix. exact same as wine variety but sequence by sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_suffix_sentence(inp_seq):\n",
    "\n",
    "    # Initialize states value to the final states of the encoder\n",
    "    states_val = encoder_model_inf.predict(inp_seq)\n",
    "\n",
    "    # Initialize the target sequence to contain the start token\n",
    "    target_seq = np.zeros((1, 1, len(vocabulary)))\n",
    "    target_seq[0, 0, char_to_idx['\\t']] = 1\n",
    "\n",
    "    # Define a variable to store the suffix sentence\n",
    "    suffix_sent = ''\n",
    "\n",
    "    # Define stop condition flag\n",
    "    stop_condition = False\n",
    "\n",
    "    # Iterate until the end token is found or maximum length of the suffix sentence is reached\n",
    "    while not stop_condition:\n",
    "\n",
    "        # Get output from decoder inference model\n",
    "        decoder_out, decoder_h, decoder_c = decoder_model_inf.predict(x=[target_seq] + states_val)\n",
    "\n",
    "        # Get most probable next character\n",
    "        max_val_index = np.argmax(decoder_out[0,-1,:])\n",
    "        sampled_output_char = idx_to_char[max_val_index]\n",
    "\n",
    "        # Append the generated char to the suffix sentence\n",
    "        suffix_sent += sampled_output_char\n",
    "\n",
    "        # Check if end token is encountered or maximum length of the suffix sentence is exceeded\n",
    "        if ((sampled_output_char == '\\n') or (len(suffix_sent) > max_len_suffix_sent)) :\n",
    "            stop_condition = True\n",
    "\n",
    "        # Add the new generated char to the existing target sequence\n",
    "        target_seq = np.zeros((1, 1, len(vocabulary)))\n",
    "        target_seq[0, 0, max_val_index] = 1\n",
    "\n",
    "        # Save state values to use in the next iteration\n",
    "        states_val = [decoder_h, decoder_c]\n",
    "\n",
    "    # Return the suffix sentence\n",
    "    return suffix_sent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prefix Sentence: t\n",
      "Suffix Sentence:  and red raspberry in the mouth there are flavors of black cherry and black plum the finish is pleasantly acidic finish\n",
      "\n",
      "Prefix Sentence: th\n",
      "Suffix Sentence:  and red raspberry in the mouth there are flavors of black cherry and black plum the finish is pleasantly acidic finish\n",
      "\n",
      "Prefix Sentence: thi\n",
      "Suffix Sentence:  and red raspberry in the mouth there are flavors of black cherry and black plum the finish is pleasantly acidic finish\n",
      "\n",
      "Prefix Sentence: this\n",
      "Suffix Sentence:  and red raspberry in the mouth there are flavors of black cherry and black plum the finish is pleasantly acidic finish\n",
      "\n",
      "Prefix Sentence: this \n",
      "Suffix Sentence:  and red raspberry in the mouth there are flavors of black cherry and black plum the finish is pleasantly acidic finish\n",
      "\n",
      "Prefix Sentence: this b\n",
      "Suffix Sentence:  and red raspberry in the mouth there are flavors of black cherry and black plum the finish is pleasantly acidic finish\n",
      "\n",
      "Prefix Sentence: this bl\n",
      "Suffix Sentence:  and red raspberry in the mouth there are flavors of black cherry and black plum the finish is pleasantly acidic finish\n",
      "\n",
      "Prefix Sentence: this ble\n",
      "Suffix Sentence:  and red raspberry in the mouth there are flavors of black cherry and black plum the finish is pleasantly acidic finish\n",
      "\n",
      "Prefix Sentence: this blen\n",
      "Suffix Sentence:  and red raspberry in the mouth there are flavors of black cherry and black plum the finish is pleasantly acidic finish\n",
      "\n",
      "Prefix Sentence: this blend\n",
      "Suffix Sentence:  and red raspberry in the mouth there are flavors of black cherry and black plum the finish is pleasantly acidic finish\n",
      "\n",
      "Prefix Sentence: this blend \n",
      "Suffix Sentence:  and red raspberry in the mouth there are flavors of black cherry and black plum the finish is pleasantly acidic finish\n",
      "\n",
      "Prefix Sentence: this blend o\n",
      "Suffix Sentence:  and red raspberry in the mouth there are flavors of black cherry and black plum the finish is pleasantly acidic finish\n",
      "\n",
      "Prefix Sentence: this blend of\n",
      "Suffix Sentence:  and red raspberry in the mouth there are flavors of black cherry and black plum the finish is pleasantly acidic finish\n",
      "\n",
      "Prefix Sentence: this blend of \n",
      "Suffix Sentence:  and red raspberry in the mouth there are flavors of black cherry and black plum the finish is pleasantly acidic finish\n",
      "\n",
      "Prefix Sentence: this blend of c\n",
      "Suffix Sentence:  and red raspberry in the mouth there are flavors of black cherry and black plum the finish is pleasantly acidic finish\n",
      "\n",
      "Prefix Sentence: this blend of ca\n",
      "Suffix Sentence:  and red raspberry in the mouth there are flavors of black cherry and black plum the finish is pleasantly acidic finish\n",
      "\n",
      "Prefix Sentence: this blend of cab\n",
      "Suffix Sentence:  and red raspberry in the mouth there are flavors of black cherry and black plum the finish is pleasantly acidic finish\n",
      "\n",
      "Prefix Sentence: this blend of cabe\n",
      "Suffix Sentence:  and red raspberry in the mouth there are flavors of black cherry and black plum the finish is pleasantly acidic finish\n",
      "\n",
      "Prefix Sentence: this blend of caber\n",
      "Suffix Sentence:  and red raspberry in the mouth there are flavors of black cherry and black plum the finish is pleasantly acidic finish\n",
      "\n",
      "Prefix Sentence: this blend of cabern\n",
      "Suffix Sentence:  and red raspberry in the mouth there are flavors of black cherry and black plum the finish is pleasantly acidic finish\n",
      "\n",
      "Prefix Sentence: this blend of caberne\n",
      "Suffix Sentence:  and red raspberry in the mouth there are flavors of black cherry and black plum the finish is pleasantly acidic finish\n",
      "\n",
      "Prefix Sentence: this blend of cabernet\n",
      "Suffix Sentence:  and red raspberry in the mouth there are flavors of black cherry and black plum the finish is pleasantly acidic finish\n",
      "\n",
      "Prefix Sentence: this blend of cabernet \n",
      "Suffix Sentence:  and red raspberry in the mouth there are flavors of black cherry and black plum the finish is pleasantly acidic finish\n",
      "\n",
      "Prefix Sentence: this blend of cabernet s\n",
      "Suffix Sentence:  and red raspberry in the mouth there are flavors of black cherry and black plum the finish is pleasantly acidic finish\n",
      "\n",
      "Prefix Sentence: this blend of cabernet sa\n",
      "Suffix Sentence:  and red raspberry in the mouth there are flavors of black cherry and black plum the finish is pleasantly acidic finish\n",
      "\n",
      "Prefix Sentence: this blend of cabernet sau\n",
      "Suffix Sentence:  and red raspberry in the mouth there are flavors of black cherry and black plum the finish is pleasantly acidic finish\n",
      "\n",
      "Prefix Sentence: this blend of cabernet sauv\n",
      "Suffix Sentence:  and red raspberry in the mouth there are flavors of black cherry and black plum the finish is pleasantly acidic finish\n",
      "\n",
      "Prefix Sentence: this blend of cabernet sauvi\n",
      "Suffix Sentence:  and red raspberry in the mouth there are flavors of black cherry and black plum the finish is pleasantly acidic finish\n",
      "\n",
      "Prefix Sentence: this blend of cabernet sauvig\n",
      "Suffix Sentence:  and red raspberry in the mouth there are flavors of black cherry and black plum the finish is pleasantly acidic finish\n",
      "\n",
      "Prefix Sentence: this blend of cabernet sauvign\n",
      "Suffix Sentence:  and red raspberry in the mouth there are flavors of black cherry and black plum the finish is pleasantly acidic finish\n",
      "\n",
      "Prefix Sentence: this blend of cabernet sauvigno\n",
      "Suffix Sentence:  and red raspberry in the mouth there are flavors of black cherry and black plum the finish is pleasantly acidic finish\n",
      "\n",
      "Prefix Sentence: this blend of cabernet sauvignon\n",
      "Suffix Sentence:  and red raspberry in the mouth there are flavors of black cherry and black plum the finish is pleasantly acidic finish\n",
      "\n",
      "Prefix Sentence: this blend of cabernet sauvignon \n",
      "Suffix Sentence:  and red raspberry in the mouth there are flavors of black cherry and black plum the finish is pleasantly acidic finish\n",
      "\n",
      "Prefix Sentence: this blend of cabernet sauvignon a\n",
      "Suffix Sentence:  and red raspberry in the mouth there are flavors of black cherry and black plum the finish is pleasantly acidic finish\n",
      "\n",
      "Prefix Sentence: this blend of cabernet sauvignon an\n",
      "Suffix Sentence:  and red raspberry in the mouth there are flavors of black cherry and black plum the finish is pleasantly acidic finish\n",
      "\n",
      "Prefix Sentence: this blend of cabernet sauvignon and\n",
      "Suffix Sentence:  and red raspberry in the mouth there are flavors of black cherry and black plum the finish is pleasantly acidic finish\n",
      "\n",
      "Prefix Sentence: this blend of cabernet sauvignon and \n",
      "Suffix Sentence:  and red raspberry in the mouth there are flavors of black cherry and black plum the finish is pleasantly acidic finish\n",
      "\n",
      "Prefix Sentence: this blend of cabernet sauvignon and m\n",
      "Suffix Sentence:  and red raspberry in the mouth there are flavors of black cherry and black plum the finish is pleasantly acidic finish\n",
      "\n",
      "Prefix Sentence: this blend of cabernet sauvignon and me\n",
      "Suffix Sentence:  and red raspberry in the mouth there are flavors of black cherry and black plum the finish is pleasantly acidic finish\n",
      "\n",
      "Prefix Sentence: this blend of cabernet sauvignon and mer\n",
      "Suffix Sentence:  and red raspberry in the mouth there are flavors of black cherry and black plum the finish is pleasantly acidic finish\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate 10 suffixes\n",
    "for seq_index in range(40):\n",
    "  \n",
    "    # Get the next tokenized sentence\n",
    "    inp_seq = input_data_prefix[seq_index:seq_index+1]\n",
    "    \n",
    "    # Generate the suffix sentence\n",
    "    suffix_sent = generate_suffix_sentence(inp_seq)\n",
    "    \n",
    "    # Print the prefix sentence\n",
    "    print('Prefix Sentence:', prefix_sentences[seq_index])\n",
    "    \n",
    "    # Print the suffix sentence\n",
    "    print('Suffix Sentence:', suffix_sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final thoughts: Even though I threw quite a bit processing power i was able to only train for about 3.5% of my overall data set. It should be possible to increase accuracy of the model by increasing model complexity, train for more epochs, train with a larger data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
